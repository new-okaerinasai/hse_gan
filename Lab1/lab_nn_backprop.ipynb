{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Нейронные сети\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4 балла)** Вам необходимо реализовать небольшой программный комплекс для обучения полносвязных нейронных сетей в рамках задачи классификации. \n",
    "\n",
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/cnn.png\" width=500>\n",
    "</img>\n",
    "\n",
    "Программный комплекс должен поддерживать минимальный набор базовых слоев и обучение сети методом стохастического градиентного спуска с подсчетом градиентов с помощью обратного распространения ошибки. На входе у каждого из слоев будет вектор $x\\in\\mathbb{R}^n$, который является выходом предыдущего слоя.\n",
    "Список поддерживаемых слоев должен включать в себя:\n",
    "\n",
    " - Полносвязный слой (Dense) с заданием количества выходных нейронов $k$:\n",
    " \n",
    " $$\\mathrm{Dense} \\equiv f\\left(\\textbf{x}\\right)=\\textbf{W}\\textbf{x}+\\textbf{b},$$\n",
    " \n",
    " где $\\textbf{W}\\in\\mathbb{R}^{(k,n)}$ - это матрица весов слоя, $\\textbf{b}\\in\\mathbb{R}^k$ - вектор смещений слоя.\n",
    " \n",
    " \n",
    " - Слой логистической нелинейности (Sigmoid), который применяется поэлементно ко всем входам:\n",
    " \n",
    "  $$\\mathrm{Sigmoid} \\equiv f\\left(\\textbf{x}\\right)=\\frac{1}{1+\\exp^{\\textbf{-x}}}$$\n",
    "  \n",
    "  \n",
    "  - [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) слой с заданием вероятности выключения нейрона ($d$). Обратите внимание, что Dropout слой ведет себя по разному в фазе обучения и в фазе применения. \n",
    "  \n",
    "  В фазе обучения независимо для каждого обучающего примера сэмплируется случайная бинарная маска $\\textbf{m}$, которая будет определять какие нейроны окажутся выключенными:\n",
    "  \n",
    "    $$\\mathrm{Dropout_{train}} \\equiv f\\left(\\textbf{x}\\right)=\\textbf{m}\\odot\\textbf{x}$$\n",
    "    $$\\textbf{m} \\in \\left\\{0,1\\right\\}^{n}$$\n",
    "    $$p\\left(m_{i}=0\\right)=d$$\n",
    "    \n",
    "  где $\\odot$ $-$ это [произведение Адамара](https://ru.wikipedia.org/wiki/Произведение_Адамара), то есть покомпонентное произведение матриц.\n",
    "  \n",
    "  В фазе применения входные значения масштабируются, чтобы сохранить уровень активности, поступающий на следующий слой:\n",
    "  \n",
    "      $$\\mathrm{Dropout_{test}}\\equiv f\\left(\\textbf{x}\\right)=\\left(1-d\\right)\\textbf{x}$$\n",
    "\n",
    "\n",
    " - Softmax-слой, объединенный с cross-entropy функцией потерь. Softmax позволяет моделировать распределение вероятностей над дискретным набором классов:\n",
    "     \n",
    "     $$\\mathrm{Softmax}_{i} \\equiv p_{i}\\left(\\textbf{x}\\right)=\\frac{e^{x_{i}}}{\\sum_{j}{e^{x_{j}}}}$$\n",
    "\n",
    "    Функция потерь cross-entropy является обобщением двухклассовой логарифмической функции потерь на случай множества классов: \n",
    "    \n",
    "    $$\\mathcal{L}=-\\sum_{i}{y_{i}\\log{p_{i}}},$$\n",
    "\n",
    "    где $\\textbf{y}=\\left[y_{1}...y_{i}...\\right]$ $-$ это вектор, размерность которого равна количеству классов и в котором все элементы нулевые, за исключением одного элемента, соответствующего правильному классу.\n",
    "    \n",
    "    Можно показать, что частная производная связки softmax слоя и cross-entropy функции потерь по входу в softmax имеет простой вид:\n",
    "    \n",
    "    $$\\frac{\\partial \\mathcal{L}}{\\partial x_{i}}=p_{i}-y_{i}$$\n",
    "    \n",
    "Каждый из слоев должен быть написан в виде отдельного класса, с методами <i>fprop</i> (прямой просчет выходов сети) и <i>bprop</i> (обратный прогон сети с нахождением градиентов весовых коэффициентов):\n",
    "\n",
    "    class Layer:\n",
    "        def fprop(self, inputs, pass_type='train'):\n",
    "            pass\n",
    "            \n",
    "        def bprop(self, outputs_deriv)\n",
    "            pass\n",
    "            \n",
    "В метод <i>bprop</i> должен передаваться градиент по выходам данного слоя. Для слоя softmax на вход метода *fprop* должны подаваться не только выходы предыдущего слоя, но и метки классов (в виде двухэлементного списка матриц в переменной inputs).\n",
    "\n",
    "При создании Dense слоя весовые коэффициенты должны инициализироваться из распределения $U(-0.01,0.01)$.\n",
    "\n",
    "Кроме того, необходимо реализовать класс нейронной сети, который должен позволять конструировать сети из любой последовательности вышеназванных слоев и обучаться на входной выборке. Класс нейронной сети должен хранить в себе упорядоченный список слоев.\n",
    "\n",
    "    class NeuralNet:\n",
    "        def add(self, layer):\n",
    "            pass\n",
    "    \n",
    "        def fit(self, X_train, y_train, batch_size, lr, num_epochs):\n",
    "            pass\n",
    "            \n",
    "        def predict(self, X):\n",
    "            pass\n",
    "            \n",
    "Можно предполагать, что softmax слой в сети один и всегда будет добавляться последним. Обучение должно происходить методом стохастического градиентного спуска с задаваемыми параметрами размера батча $K$ и шага обучения $\\eta$:\n",
    "$$\\textbf{W} \\leftarrow \\textbf{W}-\\eta \\sum_{k=1}^{K}{\\nabla_{\\textbf{W}}L\\left(\\textbf{x}^k\\right)}$$\n",
    "\n",
    "В процессе обучения должен осуществляться подсчет как значения функции потерь (cross-entropy), так и ошибки классификации (процент ошибочных топ-1 предсказаний сети). Эти ошибки должны выводиться один раз в эпоху (проход по всем обучающим примерам). Методы <i>predict</i> и <i>fit</i> должны принимать на вход numpy матрицу, в строках которой содержатся обучающие примеры.\n",
    "\n",
    "В процессе написания комплекса вам понадобится добавлять различные вспомогательные функции в исходный шаблон.\n",
    "\n",
    "Все вычисления в описанных методах должны быть реализованы в матричной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "С помощью разработанных вами классов решите задачу классификации изображений [MNIST](http://yann.lecun.com/exdb/mnist/). Ваша реализация должна работать эквивалентно по качеству [решению на основе keras](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/jqx0cwwc88advxl/MNIST_example.ipynb) при той же самой архитектуре сети и параметрах обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Т.е. в идеале нужно, чтобы было лучше, чем ##\n",
    "\n",
    "**KERAS: loss: 0.2224 - acc: 0.9318 - val_loss: 0.1799 - val_acc: 0.9466 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=8\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подгрузим данные - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import array\n",
    "import struct\n",
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "\n",
    "def download_and_unpack_file(url):\n",
    "    name_packed = os.path.basename(url)\n",
    "    name = os.path.splitext(name_packed)[0]\n",
    "    if not os.path.exists(name):\n",
    "        print(name)\n",
    "        urllib.urlretrieve(url, name_packed)\n",
    "        fi = gzip.open(name_packed, 'rb')\n",
    "        fo = codecs.open(name, 'wb')\n",
    "        fo.write(fi.read())\n",
    "        fi.close()\n",
    "        fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '32710' (I am process '36458')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/sgord1/.theano/compiledir_Darwin-16.1.0-x86_64-i386-64bit-i386-3.6.0-64/lock_dir\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-25665c5891c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msoftmaxgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_logsoftmaxgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-25665c5891c1>\u001b[0m in \u001b[0;36mcompile_logsoftmaxgrad\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlogloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msoftmaxgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_logsoftmaxgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1792\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m             defaults)\n\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[1;32m   1472\u001b[0m                         optimizer, inputs, outputs)\n\u001b[1;32m   1473\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                     \u001b[0moptimizer_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mnb_nodes_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph, start_from)\u001b[0m\n\u001b[1;32m   2495\u001b[0m                 \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m                 \u001b[0mt_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m                 \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2498\u001b[0m                 \u001b[0mtime_opts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgopt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph, start_from)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m                 \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m                 \u001b[0mnb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m             \u001b[0mloop_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[1;32m   1980\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m             \u001b[0mreplacements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/tensor/opt.py\u001b[0m in \u001b[0;36mconstant_folding\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m   6448\u001b[0m         \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6449\u001b[0m     thunk = node.op.make_thunk(node, storage_map, compute_map,\n\u001b[0;32m-> 6450\u001b[0;31m                                no_recycling=[], impl=impl)\n\u001b[0m\u001b[1;32m   6451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6452\u001b[0m     \u001b[0mrequired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 935\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 839\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1586\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Is the source code already in the cache?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[0mmodule_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_from_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36m_get_from_hash\u001b[0;34m(self, module_hash, key, keep_lock)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mkey_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_hash_to_key_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_from_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcompilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                     \u001b[0mkey_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/compilelock.py\u001b[0m in \u001b[0;36mlock_ctx\u001b[0;34m(lock_dir, keep_lock, **kw)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlock_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mget_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/compilelock.py\u001b[0m in \u001b[0;36m_get_lock\u001b[0;34m(lock_dir, **kw)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Only really try to acquire the lock if we do not have it already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_lock\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnlocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlocker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Store time at which the lock was set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sgord1/anaconda/lib/python3.6/site-packages/theano/gof/compilelock.py\u001b[0m in \u001b[0;36mlock\u001b[0;34m(tmp_dir, timeout, min_wait, max_wait, verbosity)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0mno_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mnb_wait\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "def compile_logsoftmaxgrad():\n",
    "    a =T.matrix('activations')\n",
    "    p = T.nnet.softmax(a)\n",
    "    ans = T.ivector('answers')\n",
    "    logloss = T.nnet.categorical_crossentropy(p,ans).mean()\n",
    "    grad = T.grad(logloss,a)\n",
    "    return theano.function([a,ans],grad,allow_input_downcast=True)\n",
    "softmaxgrad = compile_logsoftmaxgrad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load(samples_filename, labels_filename):\n",
    "    labels_file = codecs.open(labels_filename, 'rb')\n",
    "    magic, labels_size = struct.unpack('>II', labels_file.read(8))\n",
    "    assert magic == 2049\n",
    "    labels_array = array.array('B', labels_file.read())\n",
    "\n",
    "    samples_file = codecs.open(samples_filename, 'rb')\n",
    "    magic, samples_size, rows, cols = struct.unpack('>IIII', samples_file.read(16))\n",
    "    assert magic == 2051\n",
    "    assert labels_size == samples_size\n",
    "    samples_array = array.array('B', samples_file.read())\n",
    "\n",
    "    samples = np.zeros(shape = (samples_size, rows, cols), dtype=np.uint8)\n",
    "    labels = np.zeros(shape = (samples_size, 1), dtype = np.uint8)\n",
    "\n",
    "    for i in range(samples_size):\n",
    "        labels[i] = labels_array[i]\n",
    "        samples[i] = np.array(samples_array[i*rows*cols: (i+1)*rows*cols]).reshape(rows, cols)\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images_filename = 't10k-images-idx3-ubyte'\n",
    "test_labels_filename = 't10k-labels-idx1-ubyte'\n",
    "train_images_filename = 'train-images-idx3-ubyte'\n",
    "train_labels_filename = 'train-labels-idx1-ubyte'\n",
    "\n",
    "test_images, test_labels = load(test_images_filename, test_labels_filename)\n",
    "train_images, train_labels = load(train_images_filename, train_labels_filename)\n",
    "\n",
    "n_train = len(train_labels)\n",
    "n_test = len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "for elem in train_images:\n",
    "    X_train.append(elem.reshape((elem.shape[0]*elem.shape[1])))\n",
    "for elem in test_images:\n",
    "    X_test.append(elem.reshape((elem.shape[0]*elem.shape[1])))\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "print(np.array(X_test).shape, np.array(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADU1JREFUeJzt3V+MXPV5xvHnwfgPsVMHB3Bc262htdogojrSYkjjVkQ0\nBNxIJr1AsSrkqhRzASi0qAohVUubG5SEpFbV0i7BwlDqECUgfIEagZXWSjGWF2qMgbY21BBbxoY4\nqu20GP95e7GHaAM7vxlmzsyZ5f1+pNXMnPfMOa+O9tlzZn6z83NECEA+ZzTdAIBmEH4gKcIPJEX4\ngaQIP5AU4QeSIvxAUoQfSIrwA0mdOcidzfDMmKXZg9wlkMqb+qneiuPuZN2ewm/7SknrJE2T9K2I\nuLO0/izN1iW+vJddAijYFps7Xrfry37b0yT9raSrJF0oabXtC7vdHoDB6uU1/3JJeyLi5Yh4S9K3\nJa2qpy0A/dZL+BdK+tGEx/uqZT/H9lrbY7bHTuh4D7sDUKe+v9sfEaMRMRIRI9M1s9+7A9ChXsK/\nX9LiCY8XVcsATAG9hH+7pKW2z7c9Q9LnJW2qpy0A/db1UF9EnLR9k6Tva3yob31EPF9bZwD6qqdx\n/oh4TNJjNfUCYID4eC+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrw\nA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK\n8ANJ9TRLr+29ko5KOiXpZESM1NHUVHPmooXF+tF7ZxTr//jRB4r1P1p9U7Huf9tRrAOT6Sn8lU9F\nxBs1bAfAAHHZDyTVa/hD0hO2n7a9to6GAAxGr5f9KyJiv+3zJD1u+z8iYsvEFao/CmslaZY+0OPu\nANSlpzN/ROyvbg9JekTS8knWGY2IkYgYma6ZvewOQI26Dr/t2bY/+PZ9SVdI2lVXYwD6q5fL/vmS\nHrH99nb+KSL+uZauAPRd1+GPiJcl/UaNvQw1z2z9kuXHo2cVn/vUx77bZutzitUjXz5WrM9d2Wbz\nwCQY6gOSIvxAUoQfSIrwA0kRfiApwg8kVcd/9aVwRmGo76ll7Yby0JUzphXL0+bM7nrTp44eLa8Q\n0fW2pwrO/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOP8HTpyxUcL1S2FWnvHTr9ZrB9+9txifa72\n9LT/YbXvi5cU68/f/Hddb/u3brqhWP/Aw9u63vZUwZkfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ji\nnL9Dl3xpe9+2veeEi/Xzv7S1b/seZt+54a42a5S/Mr3kK1+7p1i/7azy1JNzH3yq630PC878QFKE\nH0iK8ANJEX4gKcIPJEX4gaQIP5BU23F+2+slfVbSoYi4qFo2T9JDkpZI2ivpmoj4Sf/a7L9X//w3\ni/UHzvtaodr998dL0vV/dUuxPk/DO87vM8u/QtM+Mr/rbU/36a6f285lZ5W3fejS8vf2z32wzm6a\n0cmZ/z5JV75j2W2SNkfEUkmbq8cAppC24Y+ILZIOv2PxKkkbqvsbJF1dc18A+qzb1/zzI+JAdf81\nSd1f2wFoRM9v+EVESGr5Asn2WttjtsdO6HivuwNQk27Df9D2Akmqbg+1WjEiRiNiJCJGpqv1ZJcA\nBqvb8G+StKa6v0bSo/W0A2BQ2obf9kZJWyX9mu19tq+TdKekT9veLel3qscAppC24/wRsbpF6fKa\ne2nU8fmnivXzpvU2ll/yod3/17dt99u+W5cX67u+0P136/f6+QmU8Qk/ICnCDyRF+IGkCD+QFOEH\nkiL8QFJ8dXdyx6+6uFjff+2JYn3Liq+22cOc99gRBoUzP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k\nxTj/+9yZFywp1l/5RPlXYPdl5amsGcefujjzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSjPMPgf9e\ndVaxPvvi8vThJf9w898U65fOmtb1tjsx+j+/2LK27oVPFZ/75CXfKtbnnlE+bijjzA8kRfiBpAg/\nkBThB5Ii/EBShB9IivADSbUd57e9XtJnJR2KiIuqZXdIul7S69Vqt0fEY/1q8v1uz+/f3cet93cc\nf8ub5frGW1a2rC36/ljxufv2lrc9d0a5jrJOzvz3SbpykuXfjIhl1Q/BB6aYtuGPiC2SDg+gFwAD\n1Mtr/ptt77S93vbZtXUEYCC6Df/dki6QtEzSAUl3tVrR9lrbY7bHTuh4l7sDULeuwh8RByPiVESc\nlnSPpOWFdUcjYiQiRqZrZrd9AqhZV+G3vWDCw89J2lVPOwAGpZOhvo2SLpN0ju19kv5C0mW2l0kK\nSXsl3dDHHgH0QdvwR8TqSRbf24de0ICXThwr1v/lf3+1WH/oDz9TrM/YWh7Lb8qfHfpYsf7rf7mn\nWD9VZzMN4RN+QFKEH0iK8ANJEX4gKcIPJEX4gaT46u7KOdvLfwefuqr14E6/v/66neteXdGy9q8v\nLS0+d/b28tdff+SvnyzWrWeL9WF15OSsYv3UGz8eUCfN4cwPJEX4gaQIP5AU4QeSIvxAUoQfSIrw\nA0kxzl85+76txfofv3Vjy9rWr/99T/t+9WT532p/7yt/Wqyfu+0nLWu/svPfu+oJ73+c+YGkCD+Q\nFOEHkiL8QFKEH0iK8ANJEX4gKcb5O/QLG7e1rP3uDyabxLhzcfp0sf7hg+XPIJSfDUyOMz+QFOEH\nkiL8QFKEH0iK8ANJEX4gKcIPJNV2nN/2Ykn3S5ovKSSNRsQ62/MkPSRpiaS9kq6JiNb/WD7VRbQs\nnTzw2gAbAerRyZn/pKRbI+JCSZdKutH2hZJuk7Q5IpZK2lw9BjBFtA1/RByIiGeq+0clvShpoaRV\nkjZUq22QdHW/mgRQv/f0mt/2Ekkfl7RN0vyIOFCVXtP4ywIAU0TH4bc9R9L3JN0SEUcm1iIiNP5+\nwGTPW2t7zPbYCR3vqVkA9eko/Lanazz4D0bEw9Xig7YXVPUFkg5N9tyIGI2IkYgYma6ZdfQMoAZt\nw2/bku6V9GJEfGNCaZOkNdX9NZIerb89AP3Syb/0flLStZKes72jWna7pDslfcf2dZJekXRNf1oE\n0A9twx8RP5TkFuXL620HwKDwCT8gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiB\npAg/kBThB5Ii/EBSTNGNlDY/fHGxvkhPDqiT5nDmB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOdH\nSr+07tli/fSA+mgSZ34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKrtOL/txZLulzRfUkgajYh1tu+Q\ndL2k16tVb4+Ix/rVKPL5kyWf6OPWf9rHbU8NnXzI56SkWyPiGdsflPS07cer2jcj4uv9aw9Av7QN\nf0QckHSgun/U9ouSFva7MQD99Z5e89teIunjkrZVi262vdP2ettnt3jOWttjtsdO6HhPzQKoT8fh\ntz1H0vck3RIRRyTdLekCScs0fmVw12TPi4jRiBiJiJHpmllDywDq0FH4bU/XePAfjIiHJSkiDkbE\nqYg4LekeScv71yaAurUNv21LulfSixHxjQnLF0xY7XOSdtXfHoB+6eTd/k9KulbSc7Z3VMtul7Ta\n9jKND//tlXRDXzoE0BedvNv/Q0mepMSYPjCF8Qk/ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ\nEX4gKcIPJEX4gaQIP5AU4QeSIvxAUo6Iwe3Mfl3SKxMWnSPpjYE18N4Ma2/D2pdEb92qs7dfjohz\nO1lxoOF/187tsYgYaayBgmHtbVj7kuitW031xmU/kBThB5JqOvyjDe+/ZFh7G9a+JHrrViO9Nfqa\nH0Bzmj7zA2hII+G3faXt/7S9x/ZtTfTQiu29tp+zvcP2WMO9rLd9yPauCcvm2X7c9u7qdtJp0hrq\n7Q7b+6tjt8P2yoZ6W2z7B7ZfsP287S9Uyxs9doW+GjluA7/stz1N0n9J+rSkfZK2S1odES8MtJEW\nbO+VNBIRjY8J2/5tScck3R8RF1XLvirpcETcWf3hPDsivjgkvd0h6VjTMzdXE8osmDiztKSrJf2B\nGjx2hb6uUQPHrYkz/3JJeyLi5Yh4S9K3Ja1qoI+hFxFbJB1+x+JVkjZU9zdo/Jdn4Fr0NhQi4kBE\nPFPdPyrp7ZmlGz12hb4a0UT4F0r60YTH+zRcU36HpCdsP217bdPNTGJ+NW26JL0maX6TzUyi7czN\ng/SOmaWH5th1M+N13XjD791WRMQySVdJurG6vB1KMf6abZiGazqauXlQJplZ+meaPHbdznhdtybC\nv1/S4gmPF1XLhkJE7K9uD0l6RMM3+/DBtydJrW4PNdzPzwzTzM2TzSytITh2wzTjdRPh3y5pqe3z\nbc+Q9HlJmxro411sz67eiJHt2ZKu0PDNPrxJ0prq/hpJjzbYy88ZlpmbW80srYaP3dDNeB0RA/+R\ntFLj7/i/JOnLTfTQoq8LJD1b/TzfdG+SNmr8MvCExt8buU7ShyVtlrRb0hOS5g1Rbw9Iek7STo0H\nbUFDva3Q+CX9Tkk7qp+VTR+7Ql+NHDc+4QckxRt+QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS\n+n+CiPjA3A5oQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1145ae358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.imshow(train_images[222], interpolation='nearest')\n",
    "print(train_labels[222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем сетку ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.uniform(-0.01, 0.01, size=(inputs.shape[1], output.size))\n",
    "   \n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        if (inputs.shape[1] != self.weight.shape[0]):\n",
    "            print \"is\", inputs.shape\n",
    "            print \"sw\", self.weight.shape\n",
    "            print 'input size error'\n",
    "            pass\n",
    "        self.inpt = inputs\n",
    "        self.outp = W.dot(self.inpt)\n",
    "        return self.outp\n",
    "\n",
    "    def bprop(self, outputs_deriv, lr): #a_вых2 = w_2 * a_вых1 + b2\n",
    "        self.weights -= lr * outputs_deriv\n",
    "        grad = self.weights.transpose().dot(outputs_deriv)\n",
    "        return grad\n",
    "\n",
    "def sigm(a):\n",
    "    return 1.0/(1.0+np.exp(-a))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        self.inpts = inputs\n",
    "        self.outp = sigm(inputs)\n",
    "        return self.outp\n",
    "    \n",
    "    def bprop(self, outputs_deriv, lr):\n",
    "        grad = sigm(inputs)*(1 - sigm(self.imputs))*outputs_deriv\n",
    "        return grad\n",
    "    \n",
    "class DropOut:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        \n",
    "    def bprop(self, outputs_deriv, lr):\n",
    "        #you code\n",
    "    \n",
    "class SoftMax:\n",
    "    def __init__(self):pass\n",
    "            \n",
    "    def fprop(self, inputs, real_target=None, pass_type='train'):\n",
    "        #you code\n",
    "    \n",
    "    def bprop(self):\n",
    "        #you code\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.loss\n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X_train, y_train, lr):\n",
    "        local_input = X_train\n",
    "        for layer in self.layers[:-1]:\n",
    "            local_input = layer.fprop(local_input)\n",
    "        \n",
    "        softmax_layer = self.layers[-1]\n",
    "        loss, accuracy = softmax_layer.fprop(local_input,y_train)\n",
    "        \n",
    "        outputs_deriv = self.layers[-1].bprop()\n",
    "        for layer in self.layers[:-1][::-1]:\n",
    "            der_so_far = layer.bprop(outputs_deriv, lr)\n",
    "            outputs_deriv = der_so_far\n",
    "            \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def global_fit(self, X_train, y_train, X_test, y_test, batch_size, lr, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_total, acc_total = 0, 0\n",
    "            for i,(Xb,yb) in enumerate(iterate_minibatches(np.array(X_train),np.array(y_train),batch_size,shuffle=True)):\n",
    "                loss_, accuracy_ = self.fit(Xb,yb, lr)\n",
    "                loss_total += loss_\n",
    "                acc_total += accuracy_\n",
    "            loss_val, accuracy_val = self.evaluate(X_test, y_test)\n",
    "            print \"epoch\", epoch\n",
    "            print \"loss\", loss_total/(i+1)\n",
    "            print \"accuracy\", acc_total/(i+1)\n",
    "            print \"ON TEST\"\n",
    "            print \"loss_val\", loss_val\n",
    "            print \"acc_val\", accuracy_val\n",
    "            print \" \"\n",
    "\n",
    "    def evaluate (self, X_test, y_test):\n",
    "        outputs_, pred_answ_ = self.predict(X_test)\n",
    "        accuracy_val = np.mean(pred_answ_ == y_test[:,0])\n",
    "        real_target = y_test\n",
    "        rt_matrix = np.zeros((len(real_target), 10))#матрица правильных ответов\n",
    "        for ind, elem in enumerate(real_target):\n",
    "            rt_matrix[ind, elem] = 1\n",
    "        \n",
    "        loss_val = - (rt_matrix*np.log(outputs_)).sum(axis=1).mean()  # LOSS!!!\n",
    "        return loss_val, accuracy_val\n",
    "\n",
    "    def predict(self, X):\n",
    "        print \"\"\n",
    "        local_input = X\n",
    "        for layer in self.layers:\n",
    "            local_input = layer.fprop(local_input, pass_type='test')\n",
    "        return local_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-hack\n",
    "\n",
    "**Сравнивать loss и градиенты с посчитанными через Theano(или другой ваш любимый фрейм). \n",
    "Если что-то не сходиться (например, у меня не сходилась прозводная L по входу в Softmax.bprop), смотреть расчеты через свое и Theano **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN=NeuralNet()\n",
    "NN.add(DenseLayer(784, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 10))\n",
    "NN.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Те же самые параметры как в решении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluate\n",
      "epoch 0\n",
      "loss 2.05262787014\n",
      "accuracy 0.579577323718\n",
      "ON TEST\n",
      "loss_val 1.78543962468\n",
      "acc_val 0.7498\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 1\n",
      "loss 1.56282987542\n",
      "accuracy 0.760049412393\n",
      "ON TEST\n",
      "loss_val 1.3384316322\n",
      "acc_val 0.8087\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 2\n",
      "loss 1.19105340545\n",
      "accuracy 0.81827590812\n",
      "ON TEST\n",
      "loss_val 1.03385704628\n",
      "acc_val 0.8516\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 3\n",
      "loss 0.943307776921\n",
      "accuracy 0.852547409188\n",
      "ON TEST\n",
      "loss_val 0.833238502639\n",
      "acc_val 0.8757\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 4\n",
      "loss 0.7789597725\n",
      "accuracy 0.873347355769\n",
      "ON TEST\n",
      "loss_val 0.698249512672\n",
      "acc_val 0.8903\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 5\n",
      "loss 0.666172983165\n",
      "accuracy 0.885450053419\n",
      "ON TEST\n",
      "loss_val 0.604033840807\n",
      "acc_val 0.8975\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 6\n",
      "loss 0.585363470827\n",
      "accuracy 0.892795138889\n",
      "ON TEST\n",
      "loss_val 0.537008876313\n",
      "acc_val 0.9033\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 7\n",
      "loss 0.526724823714\n",
      "accuracy 0.898237179487\n",
      "ON TEST\n",
      "loss_val 0.487262563126\n",
      "acc_val 0.9061\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 8\n",
      "loss 0.481951735275\n",
      "accuracy 0.903044871795\n",
      "ON TEST\n",
      "loss_val 0.448801962985\n",
      "acc_val 0.9108\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 9\n",
      "loss 0.447290179257\n",
      "accuracy 0.907869257479\n",
      "ON TEST\n",
      "loss_val 0.419259379679\n",
      "acc_val 0.913\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 10\n",
      "loss 0.418716161401\n",
      "accuracy 0.910389957265\n",
      "ON TEST\n",
      "loss_val 0.394721881324\n",
      "acc_val 0.9166\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 11\n",
      "loss 0.395462102864\n",
      "accuracy 0.913461538462\n",
      "ON TEST\n",
      "loss_val 0.374942711684\n",
      "acc_val 0.9184\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 12\n",
      "loss 0.37629107839\n",
      "accuracy 0.91639957265\n",
      "ON TEST\n",
      "loss_val 0.357984378927\n",
      "acc_val 0.9203\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 13\n",
      "loss 0.359567315687\n",
      "accuracy 0.918002136752\n",
      "ON TEST\n",
      "loss_val 0.343626288548\n",
      "acc_val 0.9219\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 14\n",
      "loss 0.344735138916\n",
      "accuracy 0.920706463675\n",
      "ON TEST\n",
      "loss_val 0.331356413514\n",
      "acc_val 0.9238\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 15\n",
      "loss 0.332673838381\n",
      "accuracy 0.922826522436\n",
      "ON TEST\n",
      "loss_val 0.320488067646\n",
      "acc_val 0.9248\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 16\n",
      "loss 0.321180210877\n",
      "accuracy 0.924495860043\n",
      "ON TEST\n",
      "loss_val 0.311187527597\n",
      "acc_val 0.9254\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 17\n",
      "loss 0.310970706796\n",
      "accuracy 0.92633213141\n",
      "ON TEST\n",
      "loss_val 0.302267344653\n",
      "acc_val 0.9286\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 18\n",
      "loss 0.302115833323\n",
      "accuracy 0.927867922009\n",
      "ON TEST\n",
      "loss_val 0.295492815751\n",
      "acc_val 0.9286\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 19\n",
      "loss 0.294595132452\n",
      "accuracy 0.929270165598\n",
      "ON TEST\n",
      "loss_val 0.28800321023\n",
      "acc_val 0.9299\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 20\n",
      "loss 0.28596220079\n",
      "accuracy 0.931390224359\n",
      "ON TEST\n",
      "loss_val 0.281888168277\n",
      "acc_val 0.9317\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 21\n",
      "loss 0.278598139547\n",
      "accuracy 0.932625534188\n",
      "ON TEST\n",
      "loss_val 0.276100234097\n",
      "acc_val 0.9323\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 22\n",
      "loss 0.272270316934\n",
      "accuracy 0.93312633547\n",
      "ON TEST\n",
      "loss_val 0.270517224066\n",
      "acc_val 0.9343\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 23\n",
      "loss 0.26652580699\n",
      "accuracy 0.934762286325\n",
      "ON TEST\n",
      "loss_val 0.265321030218\n",
      "acc_val 0.9344\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 24\n",
      "loss 0.26051827132\n",
      "accuracy 0.935830662393\n",
      "ON TEST\n",
      "loss_val 0.26117411613\n",
      "acc_val 0.9355\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 25\n",
      "loss 0.254589608019\n",
      "accuracy 0.93741653312\n",
      "ON TEST\n",
      "loss_val 0.257477052415\n",
      "acc_val 0.9354\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 26\n",
      "loss 0.251046892011\n",
      "accuracy 0.938368055556\n",
      "ON TEST\n",
      "loss_val 0.253187141777\n",
      "acc_val 0.9358\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 27\n",
      "loss 0.245193764475\n",
      "accuracy 0.93906917735\n",
      "ON TEST\n",
      "loss_val 0.249180636079\n",
      "acc_val 0.937\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 28\n",
      "loss 0.240476337251\n",
      "accuracy 0.940788595085\n",
      "ON TEST\n",
      "loss_val 0.245649921439\n",
      "acc_val 0.9376\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 29\n",
      "loss 0.236615597534\n",
      "accuracy 0.941139155983\n",
      "ON TEST\n",
      "loss_val 0.2421309205\n",
      "acc_val 0.9386\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 30\n",
      "loss 0.232786638799\n",
      "accuracy 0.942825186966\n",
      "ON TEST\n",
      "loss_val 0.238909243771\n",
      "acc_val 0.9394\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 31\n",
      "loss 0.229046841159\n",
      "accuracy 0.942725026709\n",
      "ON TEST\n",
      "loss_val 0.235968300909\n",
      "acc_val 0.9398\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 32\n",
      "loss 0.224638086604\n",
      "accuracy 0.944093883547\n",
      "ON TEST\n",
      "loss_val 0.233265179582\n",
      "acc_val 0.9404\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 33\n",
      "loss 0.222373433386\n",
      "accuracy 0.944461137821\n",
      "ON TEST\n",
      "loss_val 0.230595109821\n",
      "acc_val 0.9415\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 34\n",
      "loss 0.218395887238\n",
      "accuracy 0.945162259615\n",
      "ON TEST\n",
      "loss_val 0.227845815147\n",
      "acc_val 0.9423\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 35\n",
      "loss 0.215239133372\n",
      "accuracy 0.94671474359\n",
      "ON TEST\n",
      "loss_val 0.225506119445\n",
      "acc_val 0.9428\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 36\n",
      "loss 0.212215311259\n",
      "accuracy 0.946080395299\n",
      "ON TEST\n",
      "loss_val 0.22322966519\n",
      "acc_val 0.943\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 37\n",
      "loss 0.209559104653\n",
      "accuracy 0.947599492521\n",
      "ON TEST\n",
      "loss_val 0.2207124961\n",
      "acc_val 0.9436\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 38\n",
      "loss 0.206141395974\n",
      "accuracy 0.947966746795\n",
      "ON TEST\n",
      "loss_val 0.218819222722\n",
      "acc_val 0.9431\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 39\n",
      "loss 0.204085537373\n",
      "accuracy 0.948233840812\n",
      "ON TEST\n",
      "loss_val 0.216412656043\n",
      "acc_val 0.9442\n",
      " \n"
     ]
    }
   ],
   "source": [
    "NN.global_fit(X_train, y_train, X_test, y_test, 128, 0.1, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вывод: получилось даже на более простой сетке!**\n",
    "Решение на основе Keras:\n",
    "loss: 0.2224 - acc: 0.9318 - val_loss: 0.1799 - val_acc: 0.9466"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь с ручным градиентом и более громоздкой сетью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN=NeuralNet()\n",
    "NN.add(DenseLayer(784, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 10))\n",
    "NN.add(SoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluate\n",
      "epoch 0\n",
      "loss 1.63267958114\n",
      "accuracy 0.438151041667\n",
      "ON TEST\n",
      "loss_val 0.762013875193\n",
      "acc_val 0.7968\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 1\n",
      "loss 0.635018734859\n",
      "accuracy 0.824035122863\n",
      "ON TEST\n",
      "loss_val 0.477796910255\n",
      "acc_val 0.8754\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 2\n",
      "loss 0.500606665731\n",
      "accuracy 0.857989449786\n",
      "ON TEST\n",
      "loss_val 0.391782227217\n",
      "acc_val 0.8967\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 3\n",
      "loss 0.449348702288\n",
      "accuracy 0.870860042735\n",
      "ON TEST\n",
      "loss_val 0.355065537824\n",
      "acc_val 0.9009\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 4\n",
      "loss 0.419630688811\n",
      "accuracy 0.877620860043\n",
      "ON TEST\n",
      "loss_val 0.324826510853\n",
      "acc_val 0.9126\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 5\n",
      "loss 0.40198571523\n",
      "accuracy 0.881493723291\n",
      "ON TEST\n",
      "loss_val 0.301575796919\n",
      "acc_val 0.9178\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 6\n",
      "loss 0.383222248552\n",
      "accuracy 0.887870592949\n",
      "ON TEST\n",
      "loss_val 0.290927847831\n",
      "acc_val 0.9194\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 7\n",
      "loss 0.366368868935\n",
      "accuracy 0.891159188034\n",
      "ON TEST\n",
      "loss_val 0.270268380844\n",
      "acc_val 0.9268\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 8\n",
      "loss 0.352074236028\n",
      "accuracy 0.895799946581\n",
      "ON TEST\n",
      "loss_val 0.262155208808\n",
      "acc_val 0.9269\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 9\n",
      "loss 0.339333205385\n",
      "accuracy 0.898253872863\n",
      "ON TEST\n",
      "loss_val 0.240870593359\n",
      "acc_val 0.9315\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 10\n",
      "loss 0.324547628648\n",
      "accuracy 0.902744391026\n",
      "ON TEST\n",
      "loss_val 0.245543328873\n",
      "acc_val 0.9294\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 11\n",
      "loss 0.316735987019\n",
      "accuracy 0.905932825855\n",
      "ON TEST\n",
      "loss_val 0.226435687154\n",
      "acc_val 0.9364\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 12\n",
      "loss 0.311575591312\n",
      "accuracy 0.906617254274\n",
      "ON TEST\n",
      "loss_val 0.226286958737\n",
      "acc_val 0.9341\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 13\n",
      "loss 0.299300551392\n",
      "accuracy 0.910573584402\n",
      "ON TEST\n",
      "loss_val 0.21629544998\n",
      "acc_val 0.9346\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 14\n",
      "loss 0.290230582337\n",
      "accuracy 0.912893963675\n",
      "ON TEST\n",
      "loss_val 0.213655465012\n",
      "acc_val 0.9383\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 15\n",
      "loss 0.291738591054\n",
      "accuracy 0.910924145299\n",
      "ON TEST\n",
      "loss_val 0.206216084983\n",
      "acc_val 0.939\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 16\n",
      "loss 0.286341617576\n",
      "accuracy 0.913928952991\n",
      "ON TEST\n",
      "loss_val 0.205502276734\n",
      "acc_val 0.9388\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 17\n",
      "loss 0.279898478342\n",
      "accuracy 0.916533119658\n",
      "ON TEST\n",
      "loss_val 0.196523671377\n",
      "acc_val 0.9436\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 18\n",
      "loss 0.275058003636\n",
      "accuracy 0.919037126068\n",
      "ON TEST\n",
      "loss_val 0.19574155076\n",
      "acc_val 0.9433\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 19\n",
      "loss 0.273810182969\n",
      "accuracy 0.917451255342\n",
      "ON TEST\n",
      "loss_val 0.19513086811\n",
      "acc_val 0.9431\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 20\n",
      "loss 0.262379235106\n",
      "accuracy 0.920823317308\n",
      "ON TEST\n",
      "loss_val 0.184059464009\n",
      "acc_val 0.9458\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 21\n",
      "loss 0.254244747557\n",
      "accuracy 0.922659588675\n",
      "ON TEST\n",
      "loss_val 0.180967851594\n",
      "acc_val 0.9448\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 22\n",
      "loss 0.251074905692\n",
      "accuracy 0.924412393162\n",
      "ON TEST\n",
      "loss_val 0.176578210504\n",
      "acc_val 0.948\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 23\n",
      "loss 0.25477846036\n",
      "accuracy 0.924111912393\n",
      "ON TEST\n",
      "loss_val 0.181043452023\n",
      "acc_val 0.9467\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 24\n",
      "loss 0.250655463472\n",
      "accuracy 0.92476295406\n",
      "ON TEST\n",
      "loss_val 0.170506372987\n",
      "acc_val 0.9482\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 25\n",
      "loss 0.239064339242\n",
      "accuracy 0.927700988248\n",
      "ON TEST\n",
      "loss_val 0.170524726968\n",
      "acc_val 0.9484\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 26\n",
      "loss 0.237488437711\n",
      "accuracy 0.928485576923\n",
      "ON TEST\n",
      "loss_val 0.17297663396\n",
      "acc_val 0.9488\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 27\n",
      "loss 0.235758792444\n",
      "accuracy 0.928786057692\n",
      "ON TEST\n",
      "loss_val 0.167718302859\n",
      "acc_val 0.9486\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 28\n",
      "loss 0.235616328984\n",
      "accuracy 0.928986378205\n",
      "ON TEST\n",
      "loss_val 0.168800931468\n",
      "acc_val 0.9512\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 29\n",
      "loss 0.242430471614\n",
      "accuracy 0.925981570513\n",
      "ON TEST\n",
      "loss_val 0.168036742899\n",
      "acc_val 0.9504\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 30\n",
      "loss 0.231880394709\n",
      "accuracy 0.929754273504\n",
      "ON TEST\n",
      "loss_val 0.168259003759\n",
      "acc_val 0.949\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 31\n",
      "loss 0.233788296436\n",
      "accuracy 0.928151709402\n",
      "ON TEST\n",
      "loss_val 0.163256511348\n",
      "acc_val 0.9498\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 32\n",
      "loss 0.231654875001\n",
      "accuracy 0.928969684829\n",
      "ON TEST\n",
      "loss_val 0.161288990642\n",
      "acc_val 0.9503\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 33\n",
      "loss 0.225431514929\n",
      "accuracy 0.931657318376\n",
      "ON TEST\n",
      "loss_val 0.154602409253\n",
      "acc_val 0.9542\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 34\n",
      "loss 0.226009479888\n",
      "accuracy 0.931206597222\n",
      "ON TEST\n",
      "loss_val 0.157707963148\n",
      "acc_val 0.9538\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 35\n",
      "loss 0.219390139905\n",
      "accuracy 0.933426816239\n",
      "ON TEST\n",
      "loss_val 0.161611685268\n",
      "acc_val 0.9529\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 36\n",
      "loss 0.233296277413\n",
      "accuracy 0.929253472222\n",
      "ON TEST\n",
      "loss_val 0.164032104287\n",
      "acc_val 0.9512\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 37\n",
      "loss 0.220423486855\n",
      "accuracy 0.932675614316\n",
      "ON TEST\n",
      "loss_val 0.157851100916\n",
      "acc_val 0.9515\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 38\n",
      "loss 0.220245757976\n",
      "accuracy 0.932208199786\n",
      "ON TEST\n",
      "loss_val 0.152787232677\n",
      "acc_val 0.9553\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 39\n",
      "loss 0.218119319013\n",
      "accuracy 0.933894230769\n",
      "ON TEST\n",
      "loss_val 0.154288110811\n",
      "acc_val 0.9536\n",
      " \n"
     ]
    }
   ],
   "source": [
    "NN.global_fit(X_train, y_train, X_test, y_test, 128, 0.1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
