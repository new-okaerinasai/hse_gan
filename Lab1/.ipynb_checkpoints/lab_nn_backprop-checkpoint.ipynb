{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Нейронные сети\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4 балла)** Вам необходимо реализовать небольшой программный комплекс для обучения полносвязных нейронных сетей в рамках задачи классификации. \n",
    "\n",
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/cnn.png\" width=500>\n",
    "</img>\n",
    "\n",
    "Программный комплекс должен поддерживать минимальный набор базовых слоев и обучение сети методом стохастического градиентного спуска с подсчетом градиентов с помощью обратного распространения ошибки. На входе у каждого из слоев будет вектор $x\\in\\mathbb{R}^n$, который является выходом предыдущего слоя.\n",
    "Список поддерживаемых слоев должен включать в себя:\n",
    "\n",
    " - Полносвязный слой (Dense) с заданием количества выходных нейронов $k$:\n",
    " \n",
    " $$\\mathrm{Dense} \\equiv f\\left(\\textbf{x}\\right)=\\textbf{W}\\textbf{x}+\\textbf{b},$$\n",
    " \n",
    " где $\\textbf{W}\\in\\mathbb{R}^{(k,n)}$ - это матрица весов слоя, $\\textbf{b}\\in\\mathbb{R}^k$ - вектор смещений слоя.\n",
    " \n",
    " \n",
    " - Слой логистической нелинейности (Sigmoid), который применяется поэлементно ко всем входам:\n",
    " \n",
    "  $$\\mathrm{Sigmoid} \\equiv f\\left(\\textbf{x}\\right)=\\frac{1}{1+\\exp^{\\textbf{-x}}}$$\n",
    "  \n",
    "  \n",
    "  - [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) слой с заданием вероятности выключения нейрона ($d$). Обратите внимание, что Dropout слой ведет себя по разному в фазе обучения и в фазе применения. \n",
    "  \n",
    "  В фазе обучения независимо для каждого обучающего примера сэмплируется случайная бинарная маска $\\textbf{m}$, которая будет определять какие нейроны окажутся выключенными:\n",
    "  \n",
    "    $$\\mathrm{Dropout_{train}} \\equiv f\\left(\\textbf{x}\\right)=\\textbf{m}\\odot\\textbf{x}$$\n",
    "    $$\\textbf{m} \\in \\left\\{0,1\\right\\}^{n}$$\n",
    "    $$p\\left(m_{i}=0\\right)=d$$\n",
    "    \n",
    "  где $\\odot$ $-$ это [произведение Адамара](https://ru.wikipedia.org/wiki/Произведение_Адамара), то есть покомпонентное произведение матриц.\n",
    "  \n",
    "  В фазе применения входные значения масштабируются, чтобы сохранить уровень активности, поступающий на следующий слой:\n",
    "  \n",
    "      $$\\mathrm{Dropout_{test}}\\equiv f\\left(\\textbf{x}\\right)=\\left(1-d\\right)\\textbf{x}$$\n",
    "\n",
    "\n",
    " - Softmax-слой, объединенный с cross-entropy функцией потерь. Softmax позволяет моделировать распределение вероятностей над дискретным набором классов:\n",
    "     \n",
    "     $$\\mathrm{Softmax}_{i} \\equiv p_{i}\\left(\\textbf{x}\\right)=\\frac{e^{x_{i}}}{\\sum_{j}{e^{x_{j}}}}$$\n",
    "\n",
    "    Функция потерь cross-entropy является обобщением двухклассовой логарифмической функции потерь на случай множества классов: \n",
    "    \n",
    "    $$\\mathcal{L}=-\\sum_{i}{y_{i}\\log{p_{i}}},$$\n",
    "\n",
    "    где $\\textbf{y}=\\left[y_{1}...y_{i}...\\right]$ $-$ это вектор, размерность которого равна количеству классов и в котором все элементы нулевые, за исключением одного элемента, соответствующего правильному классу.\n",
    "    \n",
    "    Можно показать, что частная производная связки softmax слоя и cross-entropy функции потерь по входу в softmax имеет простой вид:\n",
    "    \n",
    "    $$\\frac{\\partial \\mathcal{L}}{\\partial x_{i}}=p_{i}-y_{i}$$\n",
    "    \n",
    "Каждый из слоев должен быть написан в виде отдельного класса, с методами <i>fprop</i> (прямой просчет выходов сети) и <i>bprop</i> (обратный прогон сети с нахождением градиентов весовых коэффициентов):\n",
    "\n",
    "    class Layer:\n",
    "        def fprop(self, inputs, pass_type='train'):\n",
    "            pass\n",
    "            \n",
    "        def bprop(self, outputs_deriv)\n",
    "            pass\n",
    "            \n",
    "В метод <i>bprop</i> должен передаваться градиент по выходам данного слоя. Для слоя softmax на вход метода *fprop* должны подаваться не только выходы предыдущего слоя, но и метки классов (в виде двухэлементного списка матриц в переменной inputs).\n",
    "\n",
    "При создании Dense слоя весовые коэффициенты должны инициализироваться из распределения $U(-0.01,0.01)$.\n",
    "\n",
    "Кроме того, необходимо реализовать класс нейронной сети, который должен позволять конструировать сети из любой последовательности вышеназванных слоев и обучаться на входной выборке. Класс нейронной сети должен хранить в себе упорядоченный список слоев.\n",
    "\n",
    "    class NeuralNet:\n",
    "        def add(self, layer):\n",
    "            pass\n",
    "    \n",
    "        def fit(self, X_train, y_train, batch_size, lr, num_epochs):\n",
    "            pass\n",
    "            \n",
    "        def predict(self, X):\n",
    "            pass\n",
    "            \n",
    "Можно предполагать, что softmax слой в сети один и всегда будет добавляться последним. Обучение должно происходить методом стохастического градиентного спуска с задаваемыми параметрами размера батча $K$ и шага обучения $\\eta$:\n",
    "$$\\textbf{W} \\leftarrow \\textbf{W}-\\eta \\sum_{k=1}^{K}{\\nabla_{\\textbf{W}}L\\left(\\textbf{x}^k\\right)}$$\n",
    "\n",
    "В процессе обучения должен осуществляться подсчет как значения функции потерь (cross-entropy), так и ошибки классификации (процент ошибочных топ-1 предсказаний сети). Эти ошибки должны выводиться один раз в эпоху (проход по всем обучающим примерам). Методы <i>predict</i> и <i>fit</i> должны принимать на вход numpy матрицу, в строках которой содержатся обучающие примеры.\n",
    "\n",
    "В процессе написания комплекса вам понадобится добавлять различные вспомогательные функции в исходный шаблон.\n",
    "\n",
    "Все вычисления в описанных методах должны быть реализованы в матричной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "С помощью разработанных вами классов решите задачу классификации изображений [MNIST](http://yann.lecun.com/exdb/mnist/). Ваша реализация должна работать эквивалентно по качеству [решению на основе keras](https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/jqx0cwwc88advxl/MNIST_example.ipynb) при той же самой архитектуре сети и параметрах обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Т.е. в идеале нужно, чтобы было лучше, чем ##\n",
    "\n",
    "**KERAS: loss: 0.2224 - acc: 0.9318 - val_loss: 0.1799 - val_acc: 0.9466 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=8\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подгрузим данные - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import array\n",
    "import struct\n",
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "\n",
    "def download_and_unpack_file(url):\n",
    "    name_packed = os.path.basename(url)\n",
    "    name = os.path.splitext(name_packed)[0]\n",
    "    if not os.path.exists(name):\n",
    "        print name\n",
    "        urllib.urlretrieve(url, name_packed)\n",
    "        fi = gzip.open(name_packed, 'rb')\n",
    "        fo = codecs.open(name, 'wb')\n",
    "        fo.write(fi.read())\n",
    "        fi.close()\n",
    "        fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "def compile_logsoftmaxgrad():\n",
    "    a =T.matrix('activations')\n",
    "    p = T.nnet.softmax(a)\n",
    "    ans = T.ivector('answers')\n",
    "    logloss = T.nnet.categorical_crossentropy(p,ans).mean()\n",
    "    grad = T.grad(logloss,a)\n",
    "    return theano.function([a,ans],grad,allow_input_downcast=True)\n",
    "softmaxgrad = compile_logsoftmaxgrad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load(samples_filename, labels_filename):\n",
    "    labels_file = codecs.open(labels_filename, 'rb')\n",
    "    magic, labels_size = struct.unpack('>II', labels_file.read(8))\n",
    "    assert magic == 2049\n",
    "    labels_array = array.array('B', labels_file.read())\n",
    "\n",
    "    samples_file = codecs.open(samples_filename, 'rb')\n",
    "    magic, samples_size, rows, cols = struct.unpack('>IIII', samples_file.read(16))\n",
    "    assert magic == 2051\n",
    "    assert labels_size == samples_size\n",
    "    samples_array = array.array('B', samples_file.read())\n",
    "\n",
    "    samples = np.zeros(shape = (samples_size, rows, cols), dtype=np.uint8)\n",
    "    labels = np.zeros(shape = (samples_size, 1), dtype = np.uint8)\n",
    "\n",
    "    for i in xrange(samples_size):\n",
    "        labels[i] = labels_array[i]\n",
    "        samples[i] = np.array(samples_array[i*rows*cols: (i+1)*rows*cols]).reshape(rows, cols)\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 't10k-labels-idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6afb2d803643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_labels_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train-labels-idx1-ubyte'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ddec4cbf01c4>\u001b[0m in \u001b[0;36mload\u001b[1;34m(samples_filename, labels_filename)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlabels_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmagic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>II'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mmagic\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2049\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/codecs.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[0;32m    882\u001b[0m             \u001b[1;31m# Force opening of the file in binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__builtin__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 't10k-labels-idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "test_images_filename = 't10k-images-idx3-ubyte'\n",
    "test_labels_filename = 't10k-labels-idx1-ubyte'\n",
    "train_images_filename = 'train-images-idx3-ubyte'\n",
    "train_labels_filename = 'train-labels-idx1-ubyte'\n",
    "\n",
    "test_images, test_labels = load(test_images_filename, test_labels_filename)\n",
    "train_images, train_labels = load(train_images_filename, train_labels_filename)\n",
    "\n",
    "n_train = len(train_labels)\n",
    "n_test = len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "for elem in train_images:\n",
    "    X_train.append(elem.reshape((elem.shape[0]*elem.shape[1])))\n",
    "for elem in test_images:\n",
    "    X_test.append(elem.reshape((elem.shape[0]*elem.shape[1])))\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "print np.array(X_test).shape, np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd7220b710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.imshow(train_images[8], interpolation='nearest')\n",
    "print train_labels[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем сетку ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #инициализировать начальные веса из равномерного распределения от -0.01 до 0.01\n",
    "   \n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        #print \"Dense layer fprop\"\n",
    "        if (inputs.shape[1] != self.weight.shape[0]):\n",
    "            print \"is\", inputs.shape\n",
    "            print \"sw\", self.weight.shape\n",
    "            print 'input size error'\n",
    "            pass\n",
    "        self.inpt = # you code\n",
    "        self.outp = # you code\n",
    "        return self.outp\n",
    "\n",
    "    def bprop(self, outputs_deriv, lr): #a_вых2 = w_2 * a_вых1 + b2\n",
    "        #you code\n",
    "\n",
    "def sigm(a):\n",
    "    return 1.0/(1.0+np.exp(-a))\n",
    "\n",
    "class SigmoidLayer:\n",
    "   \n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        #you code\n",
    "\n",
    "    def bprop(self, outputs_deriv, lr): \n",
    "        #you code\n",
    "    \n",
    "class DropOut:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "    def fprop(self, inputs, pass_type='train'):\n",
    "        #you code\n",
    "        \n",
    "    def bprop(self, outputs_deriv, lr):\n",
    "        #you code\n",
    "    \n",
    "class SoftMax:\n",
    "    def __init__(self):pass\n",
    "            \n",
    "    def fprop(self, inputs, real_target=None, pass_type='train'):\n",
    "        #you code\n",
    "    \n",
    "    def bprop(self):\n",
    "        #you code\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.loss\n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X_train, y_train, lr):\n",
    "        local_input = X_train\n",
    "        for layer in self.layers[:-1]:\n",
    "            local_input = layer.fprop(local_input)\n",
    "        \n",
    "        softmax_layer = self.layers[-1]\n",
    "        loss, accuracy = softmax_layer.fprop(local_input,y_train)\n",
    "        \n",
    "        outputs_deriv = self.layers[-1].bprop()\n",
    "        for layer in self.layers[:-1][::-1]:\n",
    "            der_so_far = layer.bprop(outputs_deriv, lr)\n",
    "            outputs_deriv = der_so_far\n",
    "            \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def global_fit(self, X_train, y_train, X_test, y_test, batch_size, lr, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_total, acc_total = 0, 0\n",
    "            for i,(Xb,yb) in enumerate(iterate_minibatches(np.array(X_train),np.array(y_train),batch_size,shuffle=True)):\n",
    "                loss_, accuracy_ = self.fit(Xb,yb, lr)\n",
    "                loss_total += loss_\n",
    "                acc_total += accuracy_\n",
    "            loss_val, accuracy_val = self.evaluate(X_test, y_test)\n",
    "            print \"epoch\", epoch\n",
    "            print \"loss\", loss_total/(i+1)\n",
    "            print \"accuracy\", acc_total/(i+1)\n",
    "            print \"ON TEST\"\n",
    "            print \"loss_val\", loss_val\n",
    "            print \"acc_val\", accuracy_val\n",
    "            print \" \"\n",
    "\n",
    "    def evaluate (self, X_test, y_test):\n",
    "        outputs_, pred_answ_ = self.predict(X_test)\n",
    "        accuracy_val = np.mean(pred_answ_ == y_test[:,0])\n",
    "        real_target = y_test\n",
    "        rt_matrix = np.zeros((len(real_target), 10))#матрица правильных ответов\n",
    "        for ind, elem in enumerate(real_target):\n",
    "            rt_matrix[ind, elem] = 1\n",
    "        \n",
    "        loss_val = - (rt_matrix*np.log(outputs_)).sum(axis=1).mean()  # LOSS!!!\n",
    "        return loss_val, accuracy_val\n",
    "\n",
    "    def predict(self, X):\n",
    "        print \"\"\n",
    "        local_input = X\n",
    "        for layer in self.layers:\n",
    "            local_input = layer.fprop(local_input, pass_type='test')\n",
    "        return local_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-hack\n",
    "\n",
    "**Сравнивать loss и градиенты с посчитанными через Theano(или другой ваш любимый фрейм). \n",
    "Если что-то не сходиться (например, у меня не сходилась прозводная L по входу в Softmax.bprop), смотреть расчеты через свое и Theano **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN=NeuralNet()\n",
    "NN.add(DenseLayer(784, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 10))\n",
    "NN.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Те же самые параметры как в решении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluate\n",
      "epoch 0\n",
      "loss 2.05262787014\n",
      "accuracy 0.579577323718\n",
      "ON TEST\n",
      "loss_val 1.78543962468\n",
      "acc_val 0.7498\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 1\n",
      "loss 1.56282987542\n",
      "accuracy 0.760049412393\n",
      "ON TEST\n",
      "loss_val 1.3384316322\n",
      "acc_val 0.8087\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 2\n",
      "loss 1.19105340545\n",
      "accuracy 0.81827590812\n",
      "ON TEST\n",
      "loss_val 1.03385704628\n",
      "acc_val 0.8516\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 3\n",
      "loss 0.943307776921\n",
      "accuracy 0.852547409188\n",
      "ON TEST\n",
      "loss_val 0.833238502639\n",
      "acc_val 0.8757\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 4\n",
      "loss 0.7789597725\n",
      "accuracy 0.873347355769\n",
      "ON TEST\n",
      "loss_val 0.698249512672\n",
      "acc_val 0.8903\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 5\n",
      "loss 0.666172983165\n",
      "accuracy 0.885450053419\n",
      "ON TEST\n",
      "loss_val 0.604033840807\n",
      "acc_val 0.8975\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 6\n",
      "loss 0.585363470827\n",
      "accuracy 0.892795138889\n",
      "ON TEST\n",
      "loss_val 0.537008876313\n",
      "acc_val 0.9033\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 7\n",
      "loss 0.526724823714\n",
      "accuracy 0.898237179487\n",
      "ON TEST\n",
      "loss_val 0.487262563126\n",
      "acc_val 0.9061\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 8\n",
      "loss 0.481951735275\n",
      "accuracy 0.903044871795\n",
      "ON TEST\n",
      "loss_val 0.448801962985\n",
      "acc_val 0.9108\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 9\n",
      "loss 0.447290179257\n",
      "accuracy 0.907869257479\n",
      "ON TEST\n",
      "loss_val 0.419259379679\n",
      "acc_val 0.913\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 10\n",
      "loss 0.418716161401\n",
      "accuracy 0.910389957265\n",
      "ON TEST\n",
      "loss_val 0.394721881324\n",
      "acc_val 0.9166\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 11\n",
      "loss 0.395462102864\n",
      "accuracy 0.913461538462\n",
      "ON TEST\n",
      "loss_val 0.374942711684\n",
      "acc_val 0.9184\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 12\n",
      "loss 0.37629107839\n",
      "accuracy 0.91639957265\n",
      "ON TEST\n",
      "loss_val 0.357984378927\n",
      "acc_val 0.9203\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 13\n",
      "loss 0.359567315687\n",
      "accuracy 0.918002136752\n",
      "ON TEST\n",
      "loss_val 0.343626288548\n",
      "acc_val 0.9219\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 14\n",
      "loss 0.344735138916\n",
      "accuracy 0.920706463675\n",
      "ON TEST\n",
      "loss_val 0.331356413514\n",
      "acc_val 0.9238\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 15\n",
      "loss 0.332673838381\n",
      "accuracy 0.922826522436\n",
      "ON TEST\n",
      "loss_val 0.320488067646\n",
      "acc_val 0.9248\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 16\n",
      "loss 0.321180210877\n",
      "accuracy 0.924495860043\n",
      "ON TEST\n",
      "loss_val 0.311187527597\n",
      "acc_val 0.9254\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 17\n",
      "loss 0.310970706796\n",
      "accuracy 0.92633213141\n",
      "ON TEST\n",
      "loss_val 0.302267344653\n",
      "acc_val 0.9286\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 18\n",
      "loss 0.302115833323\n",
      "accuracy 0.927867922009\n",
      "ON TEST\n",
      "loss_val 0.295492815751\n",
      "acc_val 0.9286\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 19\n",
      "loss 0.294595132452\n",
      "accuracy 0.929270165598\n",
      "ON TEST\n",
      "loss_val 0.28800321023\n",
      "acc_val 0.9299\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 20\n",
      "loss 0.28596220079\n",
      "accuracy 0.931390224359\n",
      "ON TEST\n",
      "loss_val 0.281888168277\n",
      "acc_val 0.9317\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 21\n",
      "loss 0.278598139547\n",
      "accuracy 0.932625534188\n",
      "ON TEST\n",
      "loss_val 0.276100234097\n",
      "acc_val 0.9323\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 22\n",
      "loss 0.272270316934\n",
      "accuracy 0.93312633547\n",
      "ON TEST\n",
      "loss_val 0.270517224066\n",
      "acc_val 0.9343\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 23\n",
      "loss 0.26652580699\n",
      "accuracy 0.934762286325\n",
      "ON TEST\n",
      "loss_val 0.265321030218\n",
      "acc_val 0.9344\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 24\n",
      "loss 0.26051827132\n",
      "accuracy 0.935830662393\n",
      "ON TEST\n",
      "loss_val 0.26117411613\n",
      "acc_val 0.9355\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 25\n",
      "loss 0.254589608019\n",
      "accuracy 0.93741653312\n",
      "ON TEST\n",
      "loss_val 0.257477052415\n",
      "acc_val 0.9354\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 26\n",
      "loss 0.251046892011\n",
      "accuracy 0.938368055556\n",
      "ON TEST\n",
      "loss_val 0.253187141777\n",
      "acc_val 0.9358\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 27\n",
      "loss 0.245193764475\n",
      "accuracy 0.93906917735\n",
      "ON TEST\n",
      "loss_val 0.249180636079\n",
      "acc_val 0.937\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 28\n",
      "loss 0.240476337251\n",
      "accuracy 0.940788595085\n",
      "ON TEST\n",
      "loss_val 0.245649921439\n",
      "acc_val 0.9376\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 29\n",
      "loss 0.236615597534\n",
      "accuracy 0.941139155983\n",
      "ON TEST\n",
      "loss_val 0.2421309205\n",
      "acc_val 0.9386\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 30\n",
      "loss 0.232786638799\n",
      "accuracy 0.942825186966\n",
      "ON TEST\n",
      "loss_val 0.238909243771\n",
      "acc_val 0.9394\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 31\n",
      "loss 0.229046841159\n",
      "accuracy 0.942725026709\n",
      "ON TEST\n",
      "loss_val 0.235968300909\n",
      "acc_val 0.9398\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 32\n",
      "loss 0.224638086604\n",
      "accuracy 0.944093883547\n",
      "ON TEST\n",
      "loss_val 0.233265179582\n",
      "acc_val 0.9404\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 33\n",
      "loss 0.222373433386\n",
      "accuracy 0.944461137821\n",
      "ON TEST\n",
      "loss_val 0.230595109821\n",
      "acc_val 0.9415\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 34\n",
      "loss 0.218395887238\n",
      "accuracy 0.945162259615\n",
      "ON TEST\n",
      "loss_val 0.227845815147\n",
      "acc_val 0.9423\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 35\n",
      "loss 0.215239133372\n",
      "accuracy 0.94671474359\n",
      "ON TEST\n",
      "loss_val 0.225506119445\n",
      "acc_val 0.9428\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 36\n",
      "loss 0.212215311259\n",
      "accuracy 0.946080395299\n",
      "ON TEST\n",
      "loss_val 0.22322966519\n",
      "acc_val 0.943\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 37\n",
      "loss 0.209559104653\n",
      "accuracy 0.947599492521\n",
      "ON TEST\n",
      "loss_val 0.2207124961\n",
      "acc_val 0.9436\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 38\n",
      "loss 0.206141395974\n",
      "accuracy 0.947966746795\n",
      "ON TEST\n",
      "loss_val 0.218819222722\n",
      "acc_val 0.9431\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 39\n",
      "loss 0.204085537373\n",
      "accuracy 0.948233840812\n",
      "ON TEST\n",
      "loss_val 0.216412656043\n",
      "acc_val 0.9442\n",
      " \n"
     ]
    }
   ],
   "source": [
    "NN.global_fit(X_train, y_train, X_test, y_test, 128, 0.1, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вывод: получилось даже на более простой сетке!**\n",
    "Решение на основе Keras:\n",
    "loss: 0.2224 - acc: 0.9318 - val_loss: 0.1799 - val_acc: 0.9466"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь с ручным градиентом и более громоздкой сетью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN=NeuralNet()\n",
    "NN.add(DenseLayer(784, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 512))\n",
    "NN.add(SigmoidLayer())\n",
    "NN.add(DropOut(0.3))\n",
    "NN.add(DenseLayer(512, 10))\n",
    "NN.add(SoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluate\n",
      "epoch 0\n",
      "loss 1.63267958114\n",
      "accuracy 0.438151041667\n",
      "ON TEST\n",
      "loss_val 0.762013875193\n",
      "acc_val 0.7968\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 1\n",
      "loss 0.635018734859\n",
      "accuracy 0.824035122863\n",
      "ON TEST\n",
      "loss_val 0.477796910255\n",
      "acc_val 0.8754\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 2\n",
      "loss 0.500606665731\n",
      "accuracy 0.857989449786\n",
      "ON TEST\n",
      "loss_val 0.391782227217\n",
      "acc_val 0.8967\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 3\n",
      "loss 0.449348702288\n",
      "accuracy 0.870860042735\n",
      "ON TEST\n",
      "loss_val 0.355065537824\n",
      "acc_val 0.9009\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 4\n",
      "loss 0.419630688811\n",
      "accuracy 0.877620860043\n",
      "ON TEST\n",
      "loss_val 0.324826510853\n",
      "acc_val 0.9126\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 5\n",
      "loss 0.40198571523\n",
      "accuracy 0.881493723291\n",
      "ON TEST\n",
      "loss_val 0.301575796919\n",
      "acc_val 0.9178\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 6\n",
      "loss 0.383222248552\n",
      "accuracy 0.887870592949\n",
      "ON TEST\n",
      "loss_val 0.290927847831\n",
      "acc_val 0.9194\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 7\n",
      "loss 0.366368868935\n",
      "accuracy 0.891159188034\n",
      "ON TEST\n",
      "loss_val 0.270268380844\n",
      "acc_val 0.9268\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 8\n",
      "loss 0.352074236028\n",
      "accuracy 0.895799946581\n",
      "ON TEST\n",
      "loss_val 0.262155208808\n",
      "acc_val 0.9269\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 9\n",
      "loss 0.339333205385\n",
      "accuracy 0.898253872863\n",
      "ON TEST\n",
      "loss_val 0.240870593359\n",
      "acc_val 0.9315\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 10\n",
      "loss 0.324547628648\n",
      "accuracy 0.902744391026\n",
      "ON TEST\n",
      "loss_val 0.245543328873\n",
      "acc_val 0.9294\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 11\n",
      "loss 0.316735987019\n",
      "accuracy 0.905932825855\n",
      "ON TEST\n",
      "loss_val 0.226435687154\n",
      "acc_val 0.9364\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 12\n",
      "loss 0.311575591312\n",
      "accuracy 0.906617254274\n",
      "ON TEST\n",
      "loss_val 0.226286958737\n",
      "acc_val 0.9341\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 13\n",
      "loss 0.299300551392\n",
      "accuracy 0.910573584402\n",
      "ON TEST\n",
      "loss_val 0.21629544998\n",
      "acc_val 0.9346\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 14\n",
      "loss 0.290230582337\n",
      "accuracy 0.912893963675\n",
      "ON TEST\n",
      "loss_val 0.213655465012\n",
      "acc_val 0.9383\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 15\n",
      "loss 0.291738591054\n",
      "accuracy 0.910924145299\n",
      "ON TEST\n",
      "loss_val 0.206216084983\n",
      "acc_val 0.939\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 16\n",
      "loss 0.286341617576\n",
      "accuracy 0.913928952991\n",
      "ON TEST\n",
      "loss_val 0.205502276734\n",
      "acc_val 0.9388\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 17\n",
      "loss 0.279898478342\n",
      "accuracy 0.916533119658\n",
      "ON TEST\n",
      "loss_val 0.196523671377\n",
      "acc_val 0.9436\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 18\n",
      "loss 0.275058003636\n",
      "accuracy 0.919037126068\n",
      "ON TEST\n",
      "loss_val 0.19574155076\n",
      "acc_val 0.9433\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 19\n",
      "loss 0.273810182969\n",
      "accuracy 0.917451255342\n",
      "ON TEST\n",
      "loss_val 0.19513086811\n",
      "acc_val 0.9431\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 20\n",
      "loss 0.262379235106\n",
      "accuracy 0.920823317308\n",
      "ON TEST\n",
      "loss_val 0.184059464009\n",
      "acc_val 0.9458\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 21\n",
      "loss 0.254244747557\n",
      "accuracy 0.922659588675\n",
      "ON TEST\n",
      "loss_val 0.180967851594\n",
      "acc_val 0.9448\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 22\n",
      "loss 0.251074905692\n",
      "accuracy 0.924412393162\n",
      "ON TEST\n",
      "loss_val 0.176578210504\n",
      "acc_val 0.948\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 23\n",
      "loss 0.25477846036\n",
      "accuracy 0.924111912393\n",
      "ON TEST\n",
      "loss_val 0.181043452023\n",
      "acc_val 0.9467\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 24\n",
      "loss 0.250655463472\n",
      "accuracy 0.92476295406\n",
      "ON TEST\n",
      "loss_val 0.170506372987\n",
      "acc_val 0.9482\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 25\n",
      "loss 0.239064339242\n",
      "accuracy 0.927700988248\n",
      "ON TEST\n",
      "loss_val 0.170524726968\n",
      "acc_val 0.9484\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 26\n",
      "loss 0.237488437711\n",
      "accuracy 0.928485576923\n",
      "ON TEST\n",
      "loss_val 0.17297663396\n",
      "acc_val 0.9488\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 27\n",
      "loss 0.235758792444\n",
      "accuracy 0.928786057692\n",
      "ON TEST\n",
      "loss_val 0.167718302859\n",
      "acc_val 0.9486\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 28\n",
      "loss 0.235616328984\n",
      "accuracy 0.928986378205\n",
      "ON TEST\n",
      "loss_val 0.168800931468\n",
      "acc_val 0.9512\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 29\n",
      "loss 0.242430471614\n",
      "accuracy 0.925981570513\n",
      "ON TEST\n",
      "loss_val 0.168036742899\n",
      "acc_val 0.9504\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 30\n",
      "loss 0.231880394709\n",
      "accuracy 0.929754273504\n",
      "ON TEST\n",
      "loss_val 0.168259003759\n",
      "acc_val 0.949\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 31\n",
      "loss 0.233788296436\n",
      "accuracy 0.928151709402\n",
      "ON TEST\n",
      "loss_val 0.163256511348\n",
      "acc_val 0.9498\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 32\n",
      "loss 0.231654875001\n",
      "accuracy 0.928969684829\n",
      "ON TEST\n",
      "loss_val 0.161288990642\n",
      "acc_val 0.9503\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 33\n",
      "loss 0.225431514929\n",
      "accuracy 0.931657318376\n",
      "ON TEST\n",
      "loss_val 0.154602409253\n",
      "acc_val 0.9542\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 34\n",
      "loss 0.226009479888\n",
      "accuracy 0.931206597222\n",
      "ON TEST\n",
      "loss_val 0.157707963148\n",
      "acc_val 0.9538\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 35\n",
      "loss 0.219390139905\n",
      "accuracy 0.933426816239\n",
      "ON TEST\n",
      "loss_val 0.161611685268\n",
      "acc_val 0.9529\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 36\n",
      "loss 0.233296277413\n",
      "accuracy 0.929253472222\n",
      "ON TEST\n",
      "loss_val 0.164032104287\n",
      "acc_val 0.9512\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 37\n",
      "loss 0.220423486855\n",
      "accuracy 0.932675614316\n",
      "ON TEST\n",
      "loss_val 0.157851100916\n",
      "acc_val 0.9515\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 38\n",
      "loss 0.220245757976\n",
      "accuracy 0.932208199786\n",
      "ON TEST\n",
      "loss_val 0.152787232677\n",
      "acc_val 0.9553\n",
      " \n",
      "\n",
      "evaluate\n",
      "epoch 39\n",
      "loss 0.218119319013\n",
      "accuracy 0.933894230769\n",
      "ON TEST\n",
      "loss_val 0.154288110811\n",
      "acc_val 0.9536\n",
      " \n"
     ]
    }
   ],
   "source": [
    "NN.global_fit(X_train, y_train, X_test, y_test, 128, 0.1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
